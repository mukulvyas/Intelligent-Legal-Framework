{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sCasU9PEy2bT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
        "from rouge_score import rouge_scorer\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Initialize model and tokenizer\n",
        "model_name = \"mukulvyas99/LawChatTinyLlama2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "generator = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
        "\n",
        "# Load and prepare dataset\n",
        "dataset = load_dataset(\"path_to_your_dataset\")  # Adjust dataset path/source\n",
        "\n",
        "def prepare_data(example):\n",
        "    parts = example['text'].split(\"[/INST]\")\n",
        "    ac_text = parts[0] + \"[/INST]</s>\"\n",
        "    ac_result = parts[1].strip() if len(parts) > 1 else \"\"\n",
        "    return {'ac_text': ac_text, 'ac_result': ac_result}\n",
        "\n",
        "dataset = dataset.map(prepare_data)\n",
        "\n",
        "# Generate responses and compute ROUGE scores\n",
        "predictions = []\n",
        "references = []\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "for example in dataset['train']:\n",
        "    output = generator(example['ac_text'], max_length=200)\n",
        "    model_output = output[0]['generated_text']\n",
        "    predictions.append(model_output)\n",
        "    references.append(example['ac_result'])\n",
        "\n",
        "scores = [scorer.score(ref, pred) for ref, pred in zip(references, predictions)]\n",
        "\n",
        "# Calculate and print average ROUGE scores\n",
        "average_scores = {key: sum(metric.fmeasure for metric in [score[key] for score in scores]) / len(scores) for key in ['rouge1', 'rougeL']}\n",
        "print(\"Average ROUGE-1 and ROUGE-L Scores:\", average_scores)\n"
      ],
      "metadata": {
        "id": "8U-wnZwnzC8l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}